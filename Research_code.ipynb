{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XMkUK5FXugKvENus9J-9yq1UIhavagll",
      "authorship_tag": "ABX9TyOPdwtJD5kQvEa1F9qjdYdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshJ803/shiny-octo-parakeet/blob/main/Research_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# 1. Introduce the data\n",
        "print(\"=\"*50)\n",
        "print(\"DATASET INTRODUCTION\")\n",
        "print(\"=\"*50)\n",
        "print(\n",
        "    \"• Source: Yahoo Finance via yfinance Python library\\n\"\n",
        "    \"• Companies: Apple (AAPL), Microsoft (MSFT), Amazon (AMZN), Google (GOOGL), Tesla (TSLA)\\n\"\n",
        "    \"• Time Span: 2019-01-01 to 2024-01-01\\n\"\n",
        "    \"• Features: Date, Open, High, Low, Close, Adj Close, Volume, Ticker\"\n",
        ")\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/tech-stock-2019-2024 - tech-stock-2019-2024.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "\n",
        "#Concise summary stats table\n",
        "concise_stats = df[numeric_cols + ['Ticker']].groupby('Ticker').agg(['mean', 'std', 'min', 'max']).round(2)\n",
        "print(\"\\nConcise summary stats for each Ticker:\")\n",
        "display(concise_stats)\n",
        "\n",
        "#Data normalization and standardization\n",
        "scaler_std = StandardScaler()\n",
        "scaler_minmax = MinMaxScaler()\n",
        "for col in numeric_cols:\n",
        "    df[f'{col}_z'] = scaler_std.fit_transform(df[[col]])\n",
        "    df[f'{col}_mm'] = scaler_minmax.fit_transform(df[[col]])\n",
        "\n",
        "# 4. Line plot of Close price vs. Date for each Ticker (z-score)\n",
        "plt.figure(figsize=(12, 6))\n",
        "for ticker in df['Ticker'].unique():\n",
        "    subset = df[df['Ticker'] == ticker]\n",
        "    plt.plot(subset['Date'], subset['Close_z'], label=ticker)\n",
        "plt.title(\"Standardized (z-score) Close Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Standardized Close Price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Boxplots for Close price by Ticker (z-score)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x='Ticker', y='Close_z', data=df, palette='Set2')\n",
        "plt.title(\"Standardized Close Price by Ticker\")\n",
        "plt.ylabel(\"Standardized Close Price (z-score)\")\n",
        "plt.show()\n",
        "\n",
        "# chart: mean/median volume by Ticker (normalized)\n",
        "mean_vol = df.groupby('Ticker')['Volume_mm'].mean()\n",
        "median_vol = df.groupby('Ticker')['Volume_mm'].median()\n",
        "plt.figure(figsize=(8, 5))\n",
        "mean_vol.plot(kind='bar', color='skyblue', width=0.4, position=1, label='Mean')\n",
        "median_vol.plot(kind='bar', color='navy', width=0.4, position=0, alpha=0.5, label='Median')\n",
        "plt.title(\"Normalized (0-1) Mean and Median Volume by Ticker\")\n",
        "plt.ylabel(\"Normalized Volume\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Heatmap of correlations (standardized)\n",
        "zscore_cols = [f\"{col}_z\" for col in numeric_cols]\n",
        "plt.figure(figsize=(8,6))\n",
        "corr = df[zscore_cols].corr()\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap (Standardized Features)\")\n",
        "plt.show()\n",
        "\n",
        "#Histogram of daily returns (standardized per ticker)\n",
        "for ticker in df['Ticker'].unique():\n",
        "    subset = df[df['Ticker'] == ticker].copy()\n",
        "    subset['Return'] = (subset['Close'] - subset['Open']) / subset['Open']\n",
        "    subset['Return_z'] = (subset['Return'] - subset['Return'].mean()) / subset['Return'].std()\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.histplot(subset['Return_z'], bins=30, kde=True, color='C0')\n",
        "    plt.title(f\"Standardized Daily Return Histogram: {ticker}\")\n",
        "    plt.xlabel(\"Standardized Daily Return (z-score)\")\n",
        "    plt.show()\n",
        "\n",
        "# Table of record highs/lows by Ticker\n",
        "records = []\n",
        "for ticker in df['Ticker'].unique():\n",
        "    dft = df[df['Ticker'] == ticker]\n",
        "    high_row = dft.loc[dft['Close'].idxmax()]\n",
        "    low_row = dft.loc[dft['Close'].idxmin()]\n",
        "    records.append([\n",
        "        ticker,\n",
        "        high_row['Date'].strftime(\"%Y-%m-%d\"),\n",
        "        round(high_row['Close'],2),\n",
        "        low_row['Date'].strftime(\"%Y-%m-%d\"),\n",
        "        round(low_row['Close'],2)\n",
        "    ])\n",
        "\n",
        "records_df = pd.DataFrame(records, columns=['Ticker', 'Record High Date', 'Record High Close',\n",
        "                                            'Record Low Date', 'Record Low Close'])\n",
        "print(\"Record High and Low Close Price for Each Ticker:\")\n",
        "display(records_df)\n",
        "\n",
        "#Ticker vs Record High Close (bar plot)\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x='Ticker', y='Record High Close', data=records_df, color='gold')\n",
        "plt.title(\"Record High Close Price per Ticker\")\n",
        "plt.ylabel(\"Record High Close ($)\")\n",
        "plt.xlabel(\"Ticker\")\n",
        "plt.show()\n",
        "\n",
        "# 11. Record High Date vs Record High Close (scatter plot)\n",
        "records_df['Record High Date'] = pd.to_datetime(records_df['Record High Date'])\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(\n",
        "    x='Record High Date',\n",
        "    y='Record High Close',\n",
        "    data=records_df,\n",
        "    hue='Ticker',\n",
        "    s=150\n",
        ")\n",
        "for i, row in records_df.iterrows():\n",
        "    plt.text(row['Record High Date'], row['Record High Close']+5, row['Ticker'], fontsize=10, ha='center')\n",
        "plt.title(\"When Did Each Stock Hit Its Record High Close?\")\n",
        "plt.xlabel(\"Date of Record High\")\n",
        "plt.ylabel(\"Record High Close ($)\")\n",
        "plt.show()\n",
        "\n",
        "# 12. Low-to-High Timeline Plot for Each Ticker\n",
        "plt.figure(figsize=(10,6))\n",
        "for i, row in records_df.iterrows():\n",
        "    plt.plot([pd.to_datetime(row['Record Low Date']), row['Record High Date']],\n",
        "             [row['Record Low Close'], row['Record High Close']],\n",
        "             marker='o', label=row['Ticker'])\n",
        "    plt.text(pd.to_datetime(row['Record Low Date']), row['Record Low Close']-5, f\"{row['Ticker']} Low\", ha='center', fontsize=9)\n",
        "    plt.text(row['Record High Date'], row['Record High Close']+5, f\"{row['Ticker']} High\", ha='center', fontsize=9)\n",
        "plt.title(\"Record Low to High Trajectory for Each Stock\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price ($)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#FacetGrid for Highs and Lows: Each Company's Close Series\n",
        "df['is_high'] = False\n",
        "df['is_low'] = False\n",
        "for i, row in records_df.iterrows():\n",
        "    df.loc[(df['Ticker']==row['Ticker']) & (df['Close']==row['Record High Close']), 'is_high'] = True\n",
        "    df.loc[(df['Ticker']==row['Ticker']) & (df['Close']==row['Record Low Close']), 'is_low'] = True\n",
        "\n",
        "g = sns.FacetGrid(df, col='Ticker', col_wrap=3, height=3.5, aspect=1.4, sharey=False)\n",
        "g.map_dataframe(sns.lineplot, x='Date', y='Close')\n",
        "g.map_dataframe(sns.scatterplot, x='Date', y='Close', hue='is_high', size='is_high',\n",
        "                sizes={False: 0, True: 200}, palette={True: 'red', False: 'none'}, legend=False)\n",
        "g.map_dataframe(sns.scatterplot, x='Date', y='Close', hue='is_low', size='is_low',\n",
        "                sizes={False: 0, True: 200}, palette={True: 'blue', False: 'none'}, legend=False)\n",
        "g.set_titles(\"{col_name}\")\n",
        "g.set_axis_labels(\"Date\", \"Close Price ($)\")\n",
        "g.fig.subplots_adjust(top=0.92)\n",
        "g.fig.suptitle(\"Faceted Close Price Trajectories with Record High/Low Highlighted\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "#Extreme day percent changes: Biggest daily % gain/loss per ticker\n",
        "records_vol = []\n",
        "for ticker in df['Ticker'].unique():\n",
        "    sub = df[df['Ticker'] == ticker].copy()\n",
        "    sub['Return'] = (sub['Close'] - sub['Open']) / sub['Open']\n",
        "    max_gain = sub.loc[sub['Return'].idxmax()]\n",
        "    max_drop = sub.loc[sub['Return'].idxmin()]\n",
        "    records_vol.append([\n",
        "        ticker,\n",
        "        max_gain['Date'].strftime(\"%Y-%m-%d\"),\n",
        "        round(100*max_gain['Return'],2),\n",
        "        max_drop['Date'].strftime(\"%Y-%m-%d\"),\n",
        "        round(100*max_drop['Return'],2)\n",
        "    ])\n",
        "records_vol_df = pd.DataFrame(\n",
        "    records_vol,\n",
        "    columns=['Ticker', 'Record Gain Date', 'Max Gain (%)', 'Record Drop Date', 'Max Drop (%)'])\n",
        "print(\"Biggest daily percent gain/drop per ticker (%):\")\n",
        "display(records_vol_df)"
      ],
      "metadata": {
        "id": "-qZKRaK_qPHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Forces the Colab runtime to restart so new numpy is active"
      ],
      "metadata": {
        "id": "UY1RKb5cXSeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Install dependencies: pandas-ta, transformers, torch, seaborn]\n",
        "!pip install pandas-ta transformers torch seaborn --quiet\n",
        "\n",
        "# [Mount Google Drive for file access]\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# [CSV path — update if you move/rename your file]\n",
        "news_csv_path = '/content/drive/MyDrive/sample_news  - Sheet1.csv'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# [1. LOAD NEWS]\n",
        "df_news = pd.read_csv(news_csv_path)\n",
        "df_news.columns = [col.strip() for col in df_news.columns]  # Clean up header names\n",
        "df_news['EventDate'] = pd.to_datetime(df_news['EventDate'])\n",
        "df_news['PublishDate'] = pd.to_datetime(df_news['PublishDate'])\n",
        "\n",
        "# [2. LOAD FINBERT MODEL AND TOKENIZER]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment_score(headlines):\n",
        "    \"\"\"Calculate mean sentiment score for list of headlines using FinBERT\"\"\"\n",
        "    scores = []\n",
        "    for text in headlines:\n",
        "        inputs = tokenizer(str(text), return_tensors='pt', truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits.detach().numpy()[0]\n",
        "        score = np.argmax(logits)\n",
        "        scores.append({0: -1, 1: 0, 2: 1}[score])\n",
        "    return np.mean(scores) if scores else 0.0\n",
        "\n",
        "# [3. CALCULATE TECHNICAL INDICATORS FOR ALL TICKERS]\n",
        "def add_tech_indicators(df):\n",
        "    \"\"\"Add technical indicators (SMA, EMA, MACD, RSI, etc.) for each ticker\"\"\"\n",
        "    tickers = df['Ticker'].unique()\n",
        "    results = []\n",
        "    for ticker in tickers:\n",
        "        stock_df = df[df['Ticker'] == ticker].copy().sort_values('Date')\n",
        "        stock_df = stock_df.set_index('Date')\n",
        "        stock_df['SMA_14'] = ta.sma(stock_df['Close'], length=14)\n",
        "        stock_df['EMA_14'] = ta.ema(stock_df['Close'], length=14)\n",
        "        stock_df['MACD'] = ta.macd(stock_df['Close'])['MACD_12_26_9']\n",
        "        stock_df['RSI_14'] = ta.rsi(stock_df['Close'], length=14)\n",
        "        stock_df['ROC_10'] = ta.roc(stock_df['Close'], length=10)\n",
        "        stock_df['STOCHk'] = ta.stoch(stock_df['High'], stock_df['Low'], stock_df['Close'])['STOCHk_14_3_3']\n",
        "        bbands = ta.bbands(stock_df['Close'], length=20, std=2)\n",
        "        stock_df['BB_width'] = bbands['BBU_20_2.0'] - bbands['BBL_20_2.0']\n",
        "        stock_df['ATR_14'] = ta.atr(stock_df['High'], stock_df['Low'], stock_df['Close'], length=14)\n",
        "        stock_df['OBV'] = ta.obv(stock_df['Close'], stock_df['Volume'])\n",
        "        stock_df['VWAP'] = ta.vwap(stock_df['High'], stock_df['Low'], stock_df['Close'], stock_df['Volume'])\n",
        "        stock_df = stock_df.reset_index()\n",
        "        results.append(stock_df)\n",
        "    df_ind = pd.concat(results).sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
        "    return df_ind\n",
        "\n",
        "# [4. PREP STOCK PRICE DATA, CALCULATE INDICATORS]\n",
        "df['Date'] = pd.to_datetime(df['Date'])   # df = your price dataframe, must be loaded previously\n",
        "df = df.sort_values(['Ticker', 'Date'])\n",
        "df_indicators = add_tech_indicators(df)\n",
        "df_indicators['Return'] = df_indicators.groupby('Ticker')['Close'].pct_change()\n",
        "\n",
        "# [5. AGGREGATE HEADLINES FOR EACH (TICKER, EVENTDATE)]\n",
        "# Only headlines with PublishDate == EventDate (no lookahead: strict event day!)\n",
        "event_rows = []\n",
        "for (ticker, event_date), grouped in df_news.groupby(['Ticker', 'EventDate']):\n",
        "    # Only use same-day headlines for this event window\n",
        "    window = grouped[grouped['PublishDate'] == event_date]\n",
        "    headlines = list(window['Headline'])\n",
        "    sentiment = finbert_sentiment_score(headlines)\n",
        "    # Find the matching row in stock indicators\n",
        "    price_row = df_indicators[(df_indicators['Ticker'] == ticker) & (df_indicators['Date'] == event_date)]\n",
        "    if price_row.empty:  # Skip if no matching price\n",
        "        continue\n",
        "    row = price_row.iloc[0].to_dict()\n",
        "    row['Ticker'] = ticker\n",
        "    row['EventDate'] = event_date\n",
        "    row['Sentiment'] = sentiment\n",
        "    row['HeadlineCount'] = len(headlines)\n",
        "    event_rows.append(row)\n",
        "    print(f\"{ticker} {event_date.date()}: {len(headlines)} headlines, FinBERT sentiment={sentiment:.3f}\")\n",
        "\n",
        "event_sentiment_df = pd.DataFrame(event_rows)\n",
        "\n",
        "# [6. SELECT MODEL FEATURES]\n",
        "features = [\n",
        "    'SMA_14','EMA_14','MACD','RSI_14','ROC_10','STOCHk','BB_width','ATR_14','OBV','VWAP','Sentiment'\n",
        "]\n",
        "\n",
        "# [7. TRAIN/PREDICT RANDOM FOREST PER EVENT]\n",
        "rf_results = []\n",
        "for idx, event in event_sentiment_df.iterrows():\n",
        "    ticker = event['Ticker']\n",
        "    event_date = event['EventDate']\n",
        "    # 30-day lookback window of technicals for training\n",
        "    prior_start = event_date - pd.Timedelta(days=30)\n",
        "    merged = df_indicators[(df_indicators['Ticker'] == ticker)\n",
        "                           & (df_indicators['Date'] < event_date)\n",
        "                           & (df_indicators['Date'] >= prior_start)].copy()\n",
        "    merged['Sentiment'] = event['Sentiment']  # Set event-day sentiment for training rows\n",
        "    train_df = merged.dropna(subset=features+['Return'])\n",
        "    X_train = train_df[features]\n",
        "    y_train = train_df['Return']\n",
        "    event_row = event[features]\n",
        "    X_test = pd.DataFrame([event_row])\n",
        "    y_test = np.array([event['Return']])\n",
        "    # Only predict if all data present\n",
        "    if train_df.empty or X_test.isnull().any().any():\n",
        "        continue\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)[0]\n",
        "    rf_results.append({\n",
        "        'Ticker': ticker,\n",
        "        'EventDate': event_date,\n",
        "        'ActualReturn': y_test[0],\n",
        "        'PredictedReturn': pred,\n",
        "        'HeadlineCount': event['HeadlineCount'],\n",
        "        'Sentiment': event['Sentiment']\n",
        "    })\n",
        "\n",
        "df_rf_event_preds = pd.DataFrame(rf_results)\n",
        "\n",
        "# [8. PRINT OUT METRICS AND PLOT PER TICKER]\n",
        "for ticker in df_rf_event_preds['Ticker'].unique():\n",
        "    sub = df_rf_event_preds[df_rf_event_preds['Ticker'] == ticker].copy().reset_index(drop=True)\n",
        "    sub['Residual'] = sub['ActualReturn'] - sub['PredictedReturn']\n",
        "    print(f\"\\n{ticker} Events - Random Forest Results\")\n",
        "    print(sub[['Ticker', 'EventDate', 'ActualReturn', 'PredictedReturn', 'Sentiment', 'HeadlineCount']].round(4).to_string(index=False))\n",
        "    print(\"\\nSummary statistics:\")\n",
        "    print(sub[['ActualReturn', 'PredictedReturn', 'Residual']].describe().round(4))\n",
        "    mae = mean_absolute_error(sub['ActualReturn'], sub['PredictedReturn'])\n",
        "    rmse = np.sqrt(mean_squared_error(sub['ActualReturn'], sub['PredictedReturn']))\n",
        "    r2 = r2_score(sub['ActualReturn'], sub['PredictedReturn'])\n",
        "    print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.3f}\\n\")\n",
        "    # Actual vs. predicted returns: scatterplot for visual inspection\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.regplot(x=sub['ActualReturn'], y=sub['PredictedReturn'], ci=None, color='teal', marker='o', scatter_kws={'alpha':0.7})\n",
        "    plt.plot(sub['ActualReturn'], sub['ActualReturn'], color='gray', linestyle='--', label='Perfect prediction')\n",
        "    plt.title(f'Actual vs. Predicted Return: {ticker}')\n",
        "    plt.xlabel('Actual Return')\n",
        "    plt.ylabel('Predicted Return')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hbRGaeBiYgBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}